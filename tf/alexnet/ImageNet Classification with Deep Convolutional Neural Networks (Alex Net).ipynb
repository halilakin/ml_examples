{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recently, machine learning has achieved remarkable results in systems that classify objects in images. The biggest recent advancement came from the AlexNet architecure (Krizhevsky, Sutskever, & Hinton, 2012), a huge convolutional neural network that is initially designed by LeCun at 1989, won 2012 ImageNet competition by making %40 less errors than the next best competitor. In the next years, almost all participants implemented AlexNet inspired deep neural networks and currently the models we have are quite comparable to human-level performance (Russakovsky et al., 2014).\n",
    "\n",
    "\n",
    "I will try to implement AlexNet using TensorFlow framework. Although, the original model was running on 1.2M images, I will use MNIST data for simplicity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ARCHITECTURE\n",
    "\n",
    "![AlexNet](http://sinb.github.io/images/imagenet-cnn/cnn3.png)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "| Layer Name | Layer Size | # of Neurons | # of Parameters    | # of Parameters |\n",
    "|------------|------------|--------------|--------------------|-----------------|\n",
    "| INPUT      | 224x224x3  |              |                    |                 |\n",
    "| CONV1      | 55x55x96   | 290,400      | 11 x 11 x 3 x 96   | 34,848          |\n",
    "| CONV2      | 27x27x256  | 186,624      | 5 x 5 x 96 x 256   | 614,400         |\n",
    "| CONV3      | 13x13x384  | 64,896       | 3 x 3 x 256 x 384  | 884,736         |\n",
    "| CONV4      | 13x13x384  | 64,896       | 3 x 3 x 384 x 384  | 1,327,104       |\n",
    "| CONV5      | 13x13x256  | 43,264       | 3 x 3 x 384 x 256  | 884,736         |\n",
    "| FC         | 4,096      | 4,096        | 6 x 6 x 256 x 4096 | 37,748,736      |\n",
    "| FC         | 4,096      | 4,096        | 4096 x 4096        | 16,777,216      |\n",
    "| SOFTMAX    | 1,000      | 1,000        | 4096 x 1000        | 4,096,000       |\n",
    "|            |            |              |                    |                 |\n",
    "| Total      |            | 659,272      |                    | 62,367,776      |\n",
    "\n",
    "Table : https://docs.google.com/spreadsheets/d/1uq4lmpws9pPRWFyIAfOr5-6I-xtFWwPsuf82e9nHMqM/\n",
    "\n",
    "\n",
    "* So the ALEXNET architecture has 8 layers in total, 5 of them being convolutional and 3 of them fully connected. The output of the last fully connected layer is fed to 1000-way softmax which produces class probabilities.\n",
    "* The second, third and last convolutional layer provide pooling as well.\n",
    "* The patch sizes are 11x11, 5x5, 3x3, 3x3, 3x3.\n",
    "* Another thing to recognize is input images are 224 by 224 instead of the original images which are 256 by 256. One problem with deep neural networks is it overfits a lot. Therefore they trained it on 224x224 patches extracted randomly from 256x256 images, and also their horizontal reflections.\n",
    "* One last thing is that we have 650K neurons and 60M parameters in total. That is huge. That's why using GPU's was so important back then and still now.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Download Data\n",
    "So we defined our architecture already. Let's start with downloading our data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting /tmp/data/train-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/train-labels-idx1-ubyte.gz\n",
      "Extracting /tmp/data/t10k-images-idx3-ubyte.gz\n",
      "Extracting /tmp/data/t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Import MINST data\n",
    "import input_data\n",
    "mnist = input_data.read_data_sets(\"/tmp/data/\", one_hot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Weight Initialization\n",
    "To create this model, we're going to need to create weights and biases. One should generally initialize weights with a small amount of noise for symmetry breaking, and to prevent 0 gradients. Since we're using ReLU neurons, it is also good practice to initialize them with a slightly positive initial bias to avoid \"dead neurons.\" \n",
    "\n",
    "In Alexnet paper the input images were 256x256x3. However since we are using MNIST dataset, our images are 28x28x1. So we are going to change the weights from the original paper.\n",
    "\n",
    "Original paper weights:\n",
    "\n",
    "AlexNet Parameters\n",
    "CONV1 11 x 11 x 3 x 96 (Stride 4) (MAX POOL)  \n",
    "CONV2 5 x 5 x 96 x 256 (MAX POOL)  \n",
    "CONV3 3 x 3 x 256 x 384  \n",
    "CONV4 3 x 3 x 384 x 384  \n",
    "CONV5 3 x 3 x 384 x 256 (MAX POOL)  \n",
    "FC    6 x 6 x 256 x 4096  \n",
    "FC    4096 x 4096  \n",
    "SOFTMAX 4096 x 1000  \n",
    "  \n",
    "Our Parameters  \n",
    "CONV1 3 x 3 x 1 x 64 (MAX POOL)  \n",
    "CONV2 3 x 3 x 32 x 128 (MAX POOL)  \n",
    "CONV3 3 x 3 x 64 x 256 (MAX POOL)  \n",
    "FC L*L*256 * 1024  \n",
    "FC 1024 * 1024  \n",
    "SOFTMAX 1024 * 10  \n",
    "  \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Store layers weight & bias\n",
    "weights = {\n",
    "    'wc1': tf.Variable(tf.random_normal([3, 3, 1, 64])),\n",
    "    'wc2': tf.Variable(tf.random_normal([3, 3, 64, 128])),\n",
    "    'wc3': tf.Variable(tf.random_normal([3, 3, 128, 256])),\n",
    "    'wd1': tf.Variable(tf.random_normal([4*4*256, 1024])),\n",
    "    'wd2': tf.Variable(tf.random_normal([1024, 1024])),\n",
    "    'out': tf.Variable(tf.random_normal([1024, 10]))\n",
    "}\n",
    "biases = {\n",
    "    'bc1': tf.Variable(tf.random_normal([64])),\n",
    "    'bc2': tf.Variable(tf.random_normal([128])),\n",
    "    'bc3': tf.Variable(tf.random_normal([256])),\n",
    "    'bd1': tf.Variable(tf.random_normal([1024])),\n",
    "    'bd2': tf.Variable(tf.random_normal([1024])),\n",
    "    'out': tf.Variable(tf.random_normal([10]))\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Parameters\n",
    "learning_rate = 0.001\n",
    "training_iters = 300000\n",
    "batch_size = 64\n",
    "\n",
    "# tf Graph input\n",
    "x = tf.placeholder(tf.types.float32, [None, 784])\n",
    "y = tf.placeholder(tf.types.float32, [None, 10])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Use of ReLU Nonlinearity\n",
    "One of the most important, if not the most important, novelties in this paper was the use of ReLUs. The standard way of adding nonlinearity to the network was using tanh or sigmoids. However, due to the saturating non-linearity of these functions, it takes a lot of time to train them. The non-linearity that comes with the ReLUs were good enough, but most importantly they were much faster than tanh or sigmoid. This enabled to run such a large (at that time) neural network. \n",
    "\n",
    "The convolutional layer uses 1 as the stride size, creates an image with the same size using 'SAME' padding. I am going to define this as a function so that I can use it whenever I want."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def conv2d(x, w, b):\n",
    "    return tf.nn.relu(tf.nn.bias_add(tf.nn.conv2d(x, w, strides=[1, 1, 1, 1], padding='SAME'),b))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Overlapping Pooling\n",
    "It is common to periodically insert a pooling layer right after convolution layer. This pooling layer downsamples and resizes the image using the MAX operation. The benefits : less spatial size, less computation, less parameters, control of overfitting. \n",
    "\n",
    "Pooling layers in convolutional neural networks traditionally don't overlap. If the downsizing happens 2x2 -> 1x1 then the stride is 2 and image is downsamples by 4. So, a 256x256 image becomes 128x128. However, in this paper the patches were 3x3 and the stride was 2, which creates an overlapping pooling. \n",
    "\n",
    "This scheme reduced the top-1 and top-5 error rates by 0.4% and 0.3%, respectively, as compared with the non-overlapping scheme 2x2 with stride 2, which produces output of equivalent dimensions. They also observed that models with overlapping pooling are slightly more difficult to overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_pool(x, k, s):\n",
    "    return tf.nn.max_pool(x, ksize=[1, k, k, 1], strides=[1, s, s, 1], padding='SAME')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Local Response Normalization\n",
    "Local Response Normalization is another novel part of this paper, which improved top-1 and top-5 error rates by 1.4% and 1.2%, respectively, according to the paper. I am not going to the details of the formula, however we may think of this as a way of normalizing over several adjacent kernels (these kernels are randomly adjacent) to implement lateral inhibition. I am going to use exactly the same parameters that are in the paper.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 313,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def norm(x):\n",
    "    return tf.nn.lrn(x, 4, bias=1.0, alpha=0.001 / 9.0, beta=0.75)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Training on Multiple GPUs\n",
    "Another trick in the paper was the use of multiple GPUs. They put half of the kernels on each GPU and GPUs communicated only in certain layers. \n",
    "\n",
    "This scheme reduced  top-1 and top-5 error rates by 1.7% and 1.2%, respectively, as compared with a net with half as many kernels in each convolutional layer trained on one GPU. The two-GPU net takes slightly less time to train than the one-GPU net2.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Reducing Overfitting and Dropouts\n",
    "Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. AlexNet has 60M parameters as explained above and could suffer from the same problem. The paper explains 2 primary ways of combatting this problem.\n",
    "\n",
    "#### Data Augmentation\n",
    "The first one is data augmentation which is image translations and horizontal reflections. We are not going to use these in this example. The second form of data augmentation is altering the intensities of the RGB channels in training images. Please refer to paper for extra details.\n",
    "\n",
    "#### Dropout\n",
    "Combining the predictions of many different models is usually a very successful way to reduce test errors, but since deep learning networks already take a lot of days to train, this method is unfeasible. \n",
    "\n",
    "\"Dropout\" method sets the output of each hidden neuron to zero with a probability of 0.5. The dropped out neurons dont participate in forward or backward passes. So every time a new input comes, the network develops a different architecture. These different architectures have the same weights, though. This technique prevents units from co-adapting (since a neuron cannot rely on the presence of particular other neurons) too much. Therefore, the neurons are forced to learn more robust features that are useful in conjunction with many different random subsets of the other neurons. \n",
    "\n",
    "At test time, we use all the neurons but multiply their outputs by 0.5, which is a reasonable approximation to taking the geometric mean of the predictive distributions produced by the exponentially-many dropout networks.\n",
    "We use dropout in the first two fully-connected layers, as well. Without dropout, these network can show extreme cases of overfitting. Dropout roughly doubles the number of iterations required to converge."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "keep_prob = tf.placeholder(tf.types.float32) # dropout (keep probability)\n",
    "dropout_rate = 0.8 # although the original paper had 0.5 dropout rate, we are gonna use a higher ratio because our system and dataset is much smaller\n",
    "\n",
    "def dropout(x):\n",
    "    return tf.nn.dropout(x, keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Defining ALEXNET"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def alex_net(_X, _weights, _biases, _dropout):\n",
    "    # Reshape input picture to 28x28 in order to convolve\n",
    "    _X = tf.reshape(_X, shape=[-1, 28, 28, 1])  \n",
    "\n",
    "    \n",
    "    # First Convolution Layer, followed by pooling and normalization\n",
    "    conv1 = conv2d(_X, _weights['wc1'], _biases['bc1']) # Shape: 28*28*64 \n",
    "    pool1 = max_pool(conv1, k=3, s=2) # Shape: 14*14*64\n",
    "    norm1 = norm(pool1) \n",
    "    norm1 = dropout(norm1) \n",
    "    \n",
    "    # Second Convolution Layer, followed by pooling and normalization\n",
    "    conv2 = conv2d(norm1, _weights['wc2'], _biases['bc2']) # Shape: 14*14*128\n",
    "    pool2 = max_pool(conv2, k=3, s=2) # Shape: 7*7*128\n",
    "    norm2 = norm(pool2)\n",
    "    norm2 = dropout(norm2)\n",
    "    \n",
    "    # Third Convolution Layer, followed by pooling and normalization\n",
    "    conv3 = conv2d( norm2, _weights['wc3'], _biases['bc3']) # Shape: 7*7*256\n",
    "    pool3 = max_pool( conv3, k=3, s=2) # Shape: 4*4*256\n",
    "    norm3 = norm(pool3)\n",
    "    norm3 = dropout(norm3)\n",
    "    \n",
    "    print _weights['wd1'].get_shape().as_list()[0]\n",
    "    # Fully connected layer\n",
    "    # Reshape conv3 output to fit dense layer input\n",
    "    dense1 = tf.reshape(norm3, [-1, _weights['wd1'].get_shape().as_list()[0]]) \n",
    "    # Relu activation\n",
    "    dense1 = tf.nn.relu(tf.matmul(dense1, _weights['wd1']) + _biases['bd1'], name='fc1')\n",
    "    \n",
    "    # Relu activation\n",
    "    dense2 = tf.nn.relu(tf.matmul(dense1, _weights['wd2']) + _biases['bd2'], name='fc2') \n",
    "\n",
    "    # Output, class prediction\n",
    "    out = tf.matmul(dense2, _weights['out']) + _biases['out']\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4096\n"
     ]
    }
   ],
   "source": [
    "# Construct model\n",
    "pred = alex_net(x, weights, biases, keep_prob)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Details of Learning\n",
    "The original paper used a batch size of 128 examples, momentum of 0.9 and a weight decay of 0.0005. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 317,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define loss and optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(pred, y))\n",
    "optimizer = tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(cost)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 318,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Evaluate model\n",
    "correct_pred = tf.equal(tf.argmax(pred,1), tf.argmax(y,1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.types.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 319,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initializing the variables\n",
    "init = tf.initialize_all_variables()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iter 6400, Minibatch Loss= 103547.351562, Training Accuracy= 0.15625\n",
      "Iter 12800, Minibatch Loss= 43817.253906, Training Accuracy= 0.12500\n",
      "Iter 19200, Minibatch Loss= 21805.498047, Training Accuracy= 0.25000\n",
      "Iter 25600, Minibatch Loss= 28639.246094, Training Accuracy= 0.25000\n",
      "Iter 32000, Minibatch Loss= 19440.542969, Training Accuracy= 0.20312\n",
      "Iter 38400, Minibatch Loss= 23771.638672, Training Accuracy= 0.09375\n",
      "Iter 44800, Minibatch Loss= 15034.099609, Training Accuracy= 0.17188\n",
      "Iter 51200, Minibatch Loss= 9669.911133, Training Accuracy= 0.21875"
     ]
    }
   ],
   "source": [
    "# Launch the graph\n",
    "with tf.Session() as sess:\n",
    "    sess.run(init)\n",
    "    step = 1\n",
    "    # Keep training until reach max iterations\n",
    "    while step * batch_size < training_iters:\n",
    "        batch_xs, batch_ys = mnist.train.next_batch(batch_size)\n",
    "        # Fit training using batch data\n",
    "        sess.run(optimizer, feed_dict={x: batch_xs, y: batch_ys, keep_prob: dropout_rate})\n",
    "        if step % display_step == 0:\n",
    "            # Calculate batch accuracy\n",
    "            acc = sess.run(accuracy, feed_dict={x: batch_xs, y: batch_ys, keep_prob: 1.})\n",
    "            # Calculate batch loss\n",
    "            loss = sess.run(cost, feed_dict={x: batch_xs, y: batch_ys, keep_prob: 1.})\n",
    "            print \"Iter \" + str(step*batch_size) + \", Minibatch Loss= \" \\\n",
    "                  + \"{:.6f}\".format(loss) + \", Training Accuracy= \" + \"{:.5f}\".format(acc)\n",
    "        step += 1\n",
    "    print \"Optimization Finished!\"\n",
    "    # Calculate accuracy for 256 mnist test images\n",
    "    print \"Testing Accuracy:\", sess.run(accuracy, feed_dict={x: mnist.test.images[:256], \n",
    "                                                             y: mnist.test.labels[:256], \n",
    "                                                             keep_prob: 1.})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
